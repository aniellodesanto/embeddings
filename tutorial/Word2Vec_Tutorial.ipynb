{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AKA: A Quick Introduction to Word2Vec with Python and Gensim\n",
    "@Aniello De Santo, Feb 06, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up and getting some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a resource for our data\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using this in CoLab, also run this cell, otherwise you can skip it\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/Ani/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# We are going to use the brown corpus\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by printing a few sentences out of the \"brown\" corpus, to get an idea of what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']]\n"
     ]
    }
   ],
   "source": [
    "brown_sent = brown.sents()\n",
    "print(brown_sent[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to build the whole model from scratch, we will use the Gensim library instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build an instance of the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the whole model for the brown corpus (it might take a few minutes)!\n",
    "brown_model = Word2Vec(brown_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to 'blue':\n",
      " [('red', 0.9619253873825073), ('gray', 0.9619019031524658), ('green', 0.9586248397827148)]\n"
     ]
    }
   ],
   "source": [
    "test1 = brown_model.wv.most_similar('blue')\n",
    "print(\"Most similar to 'blue':\\n\", test1[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining the model\n",
    "\n",
    "Word2Vec takes a broad range of parameters. In our example above, we only chose where to get our sentences from, and we used the *default* settings for the rest. But let's now look at a few that are most relevant (you can find a full list here: https://radimrehurek.com/gensim/models/word2vec.html):\n",
    "\n",
    "- **size**: The dimensionality of our embeddings (i.e. the length of each word vector).\n",
    "- **window**: Which words are considered contexts of the target. The size of window affects the type of similarity captured in the embeddings.\n",
    "- **negative**: The number of negative samples (incorrect training-pair instances) that are drawn for each good.\n",
    "- **sg**: Training algorithm -- 1 for skip-gram; otherwise CBOW.\n",
    "- **min_count**: Ignores all words with total frequency lower than this.\n",
    "- **iter**: Number of iterations (epochs) over the corpus.\n",
    "\n",
    "So let's now train our model by explicitly setting some of these parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the whole model (it's going to take a few minutes!)\n",
    "brown_model = Word2Vec(brown_sent, size = 300, window = 5, negative = 5, sg = 1, min_count = 5, iter = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to 'blue':\n",
      " [('silk', 0.8611888885498047), ('gray', 0.8563375473022461), ('pink', 0.850502610206604)]\n"
     ]
    }
   ],
   "source": [
    "# We can do the same test as before\n",
    "test = brown_model.wv.most_similar('blue')\n",
    "print(\"Most similar to 'blue':\\n\", test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to rely on our own **human intuitions** to decide how well the model is doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How similar is 'cup' to 'water':\n",
      " 0.5454486\n",
      "How similar is 'cup' to 'book':\n",
      " 0.56064796\n"
     ]
    }
   ],
   "source": [
    "sim = brown_model.wv.similarity(\"cup\", \"water\")\n",
    "print(\"How similar is 'cup' to 'water':\\n\", sim)\n",
    "\n",
    "sim = brown_model.wv.similarity(\"cup\", \"book\")\n",
    "print(\"How similar is 'cup' to 'book':\\n\", sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to 'child':\n",
      " [('autistic', 0.6856845617294312), ('patient', 0.666350245475769), ('fantasy', 0.6656447649002075)]\n"
     ]
    }
   ],
   "source": [
    "brown_test = brown_model.wv.most_similar('child')\n",
    "print(\"Most similar to 'child':\\n\", brown_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do more complex comparisons, but some results will be less intuitive than others!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to 'child' but dissimilar to 'person':\n",
      " [('health', 0.2888645529747009), ('high', 0.2569887638092041), ('children', 0.24687400460243225)]\n"
     ]
    }
   ],
   "source": [
    "brown_test = brown_model.wv.most_similar(positive = ['child'], negative = ['person'])\n",
    "print(\"Most similar to 'child' but dissimilar to 'person':\\n\", brown_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try a few more interesting tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which word is a mismatch in the sequence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red\n"
     ]
    }
   ],
   "source": [
    "mismatch = brown_model.wv.doesnt_match(['teacher','professor','doctor','red','athlete','runner'])\n",
    "print(mismatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe not **just** semantic relations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper\n"
     ]
    }
   ],
   "source": [
    "mismatch = brown_model.wv.doesnt_match(['running','swimming','singing','paper','reading','booking','catch'])\n",
    "print(mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between 'walk' and 'walked':\n",
      " 0.6708297\n",
      "The similarity between 'look' and 'looked':\n",
      " 0.6468835\n",
      "The similarity between 'look' and 'walk':\n",
      " 0.5292294\n"
     ]
    }
   ],
   "source": [
    "compare = brown_model.wv.similarity('walk','walked') \n",
    "print(\"The similarity between 'walk' and 'walked':\\n\", compare)\n",
    "\n",
    "compare = brown_model.wv.similarity('look','looked') \n",
    "print(\"The similarity between 'look' and 'looked':\\n\", compare)\n",
    "\n",
    "compare = brown_model.wv.similarity('look','walk') \n",
    "print(\"The similarity between 'look' and 'walk':\\n\", compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The choice of training data\n",
    "\n",
    "As for the other parameters that we looked at, the **choice of training data** (our corpus) is essential in driving model performance.\n",
    "For example, consider a very famous test case for Word2Vec: is the model able to derive the fact that \"woman\" is to \"queen\" what \"man\" is to \"king\"?\n",
    "\n",
    "We can represent this question algebraically as:\n",
    "\n",
    "$$vector(woman) +  vector(king) - vector(man) = vector(queen)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mourning', 0.7187150716781616)]\n"
     ]
    }
   ],
   "source": [
    "test = brown_model.wv.most_similar(positive=['woman','king'], negative=['man'], topn=1)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a *weird* result!\n",
    "\n",
    "However, consider the fact that the brown corpus is not too big (1M words) and it is fairly old. What would happen if we used a bigger, more recent corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with a pretrained model\n",
    "\n",
    "Luckily, NLTK includes a pre-trained model which is part of a model that is trained on 100 billion words from the Google News Dataset. The full model is from https://code.google.com/p/word2vec/ (about 3 GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to\n",
      "[nltk_data]     /Users/Ani/nltk_data...\n",
      "[nltk_data]   Package word2vec_sample is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# we need to get the data\n",
    "from nltk.data import find\n",
    "nltk.download('word2vec_sample')\n",
    "\n",
    "# we are going to use a pruned set\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time we are **not** training it from scratch, we are just loading it in (it is still going to take a bit)!\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a sanity check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('red', 0.7225173115730286),\n",
       " ('purple', 0.7134224772453308),\n",
       " ('white', 0.6606029272079468)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"blue\")[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our example once more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman','king'], negative=['man'], topn = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('France', 0.7884092330932617)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['Paris','Germany'], negative=['Berlin'], topn = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do more! Let's track **semantic shifts** (e.g. historical changes in meaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to 'gay' in the brown corpus:\n",
      " [('awfully', 0.7962538003921509), ('wonderfully', 0.7953144907951355), ('passionate', 0.79194575548172), ('ballad', 0.788561224937439), ('lonely', 0.7879823446273804)]\n"
     ]
    }
   ],
   "source": [
    "change1 = brown_model.wv.most_similar('gay')\n",
    "print(\"Most similar to 'gay' in the brown corpus:\\n\", change1[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to 'gay' in Google News:\n",
      " [('homosexual', 0.8145634531974792), ('homosexuals', 0.7562745809555054), ('lesbians', 0.7516927719116211), ('queer', 0.6972684264183044), ('Gay', 0.6740463376045227)]\n"
     ]
    }
   ],
   "source": [
    "change2 = model.most_similar('gay')\n",
    "print(\"Most similar to 'gay' in Google News:\\n\", change2[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relying on frequency patterns in human-generated data to make inferences has some problems..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between 'she' and 'engineer':\n",
      " 0.0032564793\n",
      "The similarity between 'he' and 'engineer':\n",
      " 0.107617\n"
     ]
    }
   ],
   "source": [
    "compare1 = model.similarity('she','engineer')\n",
    "print(\"The similarity between 'she' and 'engineer':\\n\", compare1)\n",
    "\n",
    "compare2 = model.similarity('he','engineer')\n",
    "print(\"The similarity between 'he' and 'engineer':\\n\", compare2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between 'woman' and 'nurse':\n",
      " 0.44135568\n",
      "The similarity between 'man' and 'nurse':\n",
      " 0.25472283\n"
     ]
    }
   ],
   "source": [
    "compare1 = model.similarity('woman','nurse')\n",
    "print(\"The similarity between 'woman' and 'nurse':\\n\", compare1)\n",
    "\n",
    "compare2 = model.similarity('man','nurse')\n",
    "print(\"The similarity between 'man' and 'nurse':\\n\", compare2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between 'black' and 'criminal':\n",
      " 0.08380781\n",
      "The similarity between 'white' and 'criminal':\n",
      " 0.04107798\n"
     ]
    }
   ],
   "source": [
    "compare1 = model.similarity('black','criminal') \n",
    "print(\"The similarity between 'black' and 'criminal':\\n\", compare1)\n",
    "\n",
    "compare2 = model.similarity('white','criminal') \n",
    "print(\"The similarity between 'white' and 'criminal':\\n\", compare2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
